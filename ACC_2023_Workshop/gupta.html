<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Learning in Infinite Dimensional Spaces</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="program.html">Program</a></div>
<div class="menu-item"><a href="contact.html">Contact</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Learning in Infinite Dimensional Spaces</h1>
</div>
<h3>Speaker</h3>
<p>Abhishek Gupta
</p>
<h3>Affiliation</h3>
<p>The Ohio State University
</p>
<h3>Abstract</h3>
<p>In many areas of learning, one is interested in learning a function in an infinite dimensional function space that is a fixed point of an operator. For example, in reinforcement learning, we are interested in computing the fixed point of a Bellman operator. Other example includes functional data analysis and risk-sensitive Markov decision processes (MDPs). In data-driven learning, such problems can be modeled as computation of an approximation of the fixed point of that operator using samples. In this presentation, we will talk about a general theoretical framework that builds upon the random operator theory to yield convergence guarantees for this class of problems. In particular, we view learning algorithm as a random operator acting on a Banach space that yields a Markov chain. We will derive various properties of such a Markov chain and its relationship to the fixed point of the operator.</p>
</td>
</tr>
</table>
</body>
</html>
